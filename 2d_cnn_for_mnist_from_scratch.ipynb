{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D CNN for MNIST digit recognition from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mnist_sample.png\" style=\"width:30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using TF 1.13.1\n",
      "We're using Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf  # !pip install tensorflow\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings('ignore')  ## Never print matching warnings\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  ## Control logging by filtering out ERROR logs\n",
    "\n",
    "print(\"We're using TF\", tf.__version__)\n",
    "print(\"We're using Python\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the data\n",
    "\n",
    "In this task we have 55000 28x28 images of digits from 0 to 9.\n",
    "We will train a classifier on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "X_train = mnist.train.images  ## numpy array of shape (55000, 784) i.e pixels pixels [0,1.0]\n",
    "y_train = mnist.train.labels  ## numpy array of shape (55000, 10) i.e. one-hot encoded labels\n",
    "# one-hot coding have 1 at corresponding position and zeros else\n",
    "# examples:\n",
    "#  0 is (1, 0, 0, 0, 0, 0, 0, 0, 0, 0,)\n",
    "#  1 is (0, 1, 0, 0, 0, 0, 0, 0, 0, 0,)\n",
    "#  2 is (0, 0, 1, 0, 0, 0, 0, 0, 0, 0,)\n",
    "# ...\n",
    "#  9 is (0, 0, 0, 0, 0, 0, 0, 0, 0, 1,)\n",
    "\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train [shape (55000, 784)]  whole sample:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train [shape (55000, 10)] 1 samples:\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Images are already flattened which means that our linear model implementation now is simplified\n",
    "print(\"X_train [shape %s]  whole sample:\" % (str(X_train.shape)))\n",
    "def show_img(sample):\n",
    "    pixels = np.array(sample, dtype = 'float32')\n",
    "    pixels = pixels.reshape((28,28))\n",
    "    plt.imshow(pixels)\n",
    "    plt.show()\n",
    "show_img(X_train[1, :])\n",
    "print(\"y_train [shape %s] 1 samples:\\n\" % (str(y_train.shape)), y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with 2D CNN\n",
    "\n",
    "Here we will train a linear classifier $\\vec{x} \\rightarrow y$ with SGD using low-level TensorFlow.\n",
    "\n",
    "First, we need to calculate a logit (a linear transformation) $z_k$ for each class: \n",
    "$$z_k = \\vec{x} \\cdot \\vec{w_k} + b_k \\quad k = 0..9$$\n",
    "```python\n",
    "logits = X @ W1 + b1  ### logits for input_X, resulting shape should be [input_X.shape[0], 256]\n",
    "```\n",
    "Then, we transform these logits $z_k$ to a valid probabilities $p_k$ with softmax: \n",
    "$$p_k = \\frac{e^{z_k}}{\\sum_{i=0}^{9}{e^{z_i}}} \\quad k = 0..9$$\n",
    "```python\n",
    "def softmax(z):  ## this approach provides numerical stability\n",
    "    \"\"\"Compute softmax values for each sets of scores in z.\"\"\"\n",
    "    e = tf.exp(z - tf.reduce_max(z))\n",
    "    return e / tf.reduce_sum(e)\n",
    "```\n",
    "In order to avoid numerical overflow we use numerically stable sigmoid function:\n",
    "$$p_k=\\frac{1}{1+e^{-z_k}}=\\frac{e^{z_k}}{1 + e^{z_k}} \\quad k = 0..9$$\n",
    "where the distributions $[z_k,0]$ and $[0,âˆ’z_k]$ are equivalent.\n",
    "```python\n",
    "def sigmoid(z):\n",
    "    \"\"\"Numerically stable sigmoid function.\"\"\"\n",
    "    # if z is less than zero then z will be small, denom can't be\n",
    "    # zero because it's 1+z.\n",
    "    return tf.where(z >= 0, 1 / (1 + tf.exp(-z)), tf.exp(z) / (1 + tf.exp(z))) \n",
    "```\n",
    "\n",
    "Finally, We use a cross-entropy loss to train our multi-class classifier:\n",
    "$$\\text{cross-entropy}(y, p) = -\\sum_{k=0}^{9}{\\log(p_k)[y = k]}$$ \n",
    "\n",
    "where \n",
    "$$\n",
    "[x]=\\begin{cases}\n",
    "       1, \\quad \\text{if $x$ is true} \\\\\n",
    "       0, \\quad \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Cross-entropy minimization pushes $p_k$ close to 1 when $y = k$, which is what we want.\n",
    "```python\n",
    "# The loss should be a scalar number: average loss over all the objects with tf.reduce_mean().\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(softmax(y_pred)), reduction_indices=[1]))\n",
    "```\n",
    "Here's our plan:\n",
    "* We use a matrix placeholder for flattened `X_train` (28x28 = 784)\n",
    "* Convert `y_train` to one-hot (already done) encoded vectors that are needed for cross-entropy\n",
    "* We use a shared variable `W` for all weights (a column $\\vec{w_k}$ per class) and `b` for all biases.\n",
    "* Our aim is: $\\text{test accuracy} \\geq 0.95$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for the input data\n",
    "# first shape is None means that the batch size will be specified at runtime\n",
    "X = tf.placeholder(tf.float32, shape=(None, 784), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model parameters: W and b\n",
    "# The initializer Xavier Glorot and Yoshua Bengio (2010) is designed to keep the \n",
    "# Scale of the gradients roughly the same in all layers\n",
    "# Zeros_initializer is used for initializing the bias with zero\n",
    "\n",
    "\n",
    "# 1st hidden layer\n",
    "W1 = tf.get_variable(\"W1\", shape=(784, 256), dtype=tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.get_variable(\"b1\", shape=(256), dtype=tf.float32, initializer = tf.zeros_initializer)\n",
    "\n",
    "# 2nd hidden layer\n",
    "W2 = tf.get_variable(\"W2\", shape=(256, 10), dtype=tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable(\"b2\", shape=(10), dtype=tf.float32, initializer = tf.zeros_initializer)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Numerically stable sigmoid function.\"\"\"\n",
    "    # if z is less than zero then z will be small, denom can't be\n",
    "    # zero because it's 1+z.\n",
    "    return tf.where(z >= 0, 1 / (1 + tf.exp(-z)), tf.exp(z) / (1 + tf.exp(z))) \n",
    "\n",
    "### Compute predictions\n",
    "logits = X @ W1 + b1  ### logits for input_X, resulting shape should be [input_X.shape[0], 256]\n",
    "probas = tf.nn.sigmoid(logits)  ### apply tf.nn.softmax to logits\n",
    "y_pred = probas @ W2 + b2  ### logits for probabilities, resulting shape should be [256, 10]\n",
    "\n",
    "def softmax(z):  ## this approach provides numerical stability\n",
    "    \"\"\"Compute softmax values for each sets of scores in z.\"\"\"\n",
    "    e = tf.exp(z - tf.reduce_max(z))\n",
    "    return e / tf.reduce_sum(e)\n",
    "\n",
    "### Cross-Entropy loss \n",
    "# Hint: use tf.reduce_mean, tf.reduce_sum for reduction by specified axis\n",
    "# The loss should be a scalar number: average loss over all the objects with tf.reduce_mean().\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(softmax(y_pred)), reduction_indices=[1]))\n",
    "#tf.nn.softmax(y_pred)\n",
    "# We use a default tf.train.AdamOptimizer to get an SGD step\n",
    "lr = 0.01\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "step = optimizer.minimize(cost)  ### optimizer step that minimizes the loss (cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = tf.gradients(cost, tf.trainable_variables()) # take gradient of ALL TRAINABLES variables\n",
    "grad_list = list(zip(grads, tf.trainable_variables()))\n",
    "\n",
    "update = optimizer.apply_gradients(grads_and_vars=grad_list) # don't rename this op\n",
    "\n",
    "### Apply tf.argmax to find a class index with highest probability\n",
    "pred_classes = tf.argmax(y_pred, axis=1)  ## class index with highest probability for the predictions\n",
    "true_classes = tf.argmax(y, axis=1)  ## class index with highest probability for the true outcome\n",
    "pred = tf.cast(tf.equal(pred_classes, true_classes), dtype=tf.float32)\n",
    "acc = tf.reduce_mean(pred) # don't rename this op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cost after 10 epochs: 6.3396\n",
      "Test cost after 30 epochs: 6.2765\n",
      "Test cost after 50 epochs: 6.2594\n",
      "Test cost after 70 epochs: 6.2546\n",
      "Test cost after 90 epochs: 6.2511\n",
      "Test cost after 110 epochs: 6.2466\n",
      "Test cost after 130 epochs: 6.2471\n",
      "\n",
      "OPTIMIZATION DONE!\n",
      "Score = 0.999800\n"
     ]
    }
   ],
   "source": [
    "# We use in this part only train data! and never use test data\n",
    "# We should split the data into mini-batches.\n",
    "\n",
    "# The smaller the batch the less accurate the estimate of the gradient will be. And limited by your GPU memory\n",
    "BATCH_SIZE = 512  ## Number of samples that will be propagated through the network\n",
    "EPOCHS= 140 # 1 Epoch is view of all elements in dataset 1 time\n",
    "BATCH_NUM = int(X_train.shape[0]/BATCH_SIZE)  # Here you loose the rest of batch you can add it\n",
    "\n",
    "# Add an Op to initialize global variables.\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()  ## Launch the graph in a session.\n",
    "sess.run(init_op)  ## # Run the Op that initializes global variables.\n",
    "\n",
    "for epoch in range(EPOCHS):  # we finish an epoch when we've looked at all training samples\n",
    "    err = 0\n",
    "    for batch in range(BATCH_NUM):\n",
    "        # extract a batch\n",
    "        batch_X = X_train[batch*BATCH_SIZE : batch*BATCH_SIZE + BATCH_SIZE]\n",
    "        batch_y = y_train[batch*BATCH_SIZE : batch*BATCH_SIZE + BATCH_SIZE]\n",
    "        \n",
    "        # a key operation. Run 'operations' and place data in placeholders 'X' and 'y'\n",
    "        _, c = sess.run([update, cost], feed_dict={X:batch_X, y:batch_y})\n",
    "        err += c\n",
    "    err /= BATCH_NUM\n",
    "    if epoch%20 == 9:\n",
    "            print(\"Test cost after %d epochs: %.4f\" %(epoch+1, err))\n",
    "    \n",
    "print(\"\\nOPTIMIZATION DONE!\")\n",
    "acc_train = sess.run([acc], feed_dict={X: X_train, y: y_train})\n",
    "print('Score = %f' %acc_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final output of accuracy is ~ 99.99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.invert(Image.open(\"images/test_img.png\").convert('L')).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image: 2\n"
     ]
    }
   ],
   "source": [
    "prediction = sess.run(tf.argmax(y_pred, 1), feed_dict={X: [img]})\n",
    "print (\"Prediction for test image:\", np.squeeze(prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
